{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from pathlib import Path\n",
    "\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =[PATH]\n",
    "direc=[]\n",
    "for root, directories, files in os.walk(path):\n",
    "    for directory in directories:\n",
    "        direc.append(os.path.join(root, directory))\n",
    "#open the directory and save files into a list too\n",
    "file_sub=[]\n",
    "for i in direc:\n",
    "    path=i\n",
    "    for files in os.walk(path):\n",
    "        for file in files:\n",
    "            file_sub.append(file)\n",
    "        #filter out empty str and unneeded info\n",
    "        file_sub=list(filter(None, file_sub))\n",
    "file_sub=file_sub[1::2]\n",
    "\n",
    "files_list=[]\n",
    "for i in direc:\n",
    "    path=i\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for name in files:\n",
    "            if name.endswith((\".json\")):\n",
    "                full_path = os.path.join(root, name)\n",
    "                files_list.append(full_path)\n",
    "\n",
    "uniquefiles=[]\n",
    "for i in files_list:\n",
    "    if \".json\" in str(i):\n",
    "        file=re.findall(r\"^(.*?)\\.json\", i)[0]\n",
    "        uniquefiles.append(file)\n",
    "\n",
    "# print(uniquefiles)\n",
    "# for i in uniquefiles:\n",
    "#     #open key level dataset\n",
    "#     print(i)\n",
    "#     df=pd.read_csv(i+\"_keylevel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =[PATH]\n",
    "direc=[]\n",
    "for root, directories, files in os.walk(path):\n",
    "    for directory in directories:\n",
    "        direc.append(os.path.join(root, directory))\n",
    "#open the directory and save files into a list too\n",
    "file_sub=[]\n",
    "for i in direc:\n",
    "    path=i\n",
    "    for files in os.walk(path):\n",
    "        for file in files:\n",
    "            file_sub.append(file)\n",
    "        #filter out empty str and unneeded info\n",
    "        file_sub=list(filter(None, file_sub))\n",
    "file_sub=file_sub[1::2]\n",
    "\n",
    "files_list=[]\n",
    "for i in direc:\n",
    "    path=i\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for name in files:\n",
    "            if name.endswith((\".json\")):\n",
    "                full_path = os.path.join(root, name)\n",
    "                files_list.append(full_path)\n",
    "\n",
    "uniquefiles=[]\n",
    "for i in files_list:\n",
    "    if \".json\" in str(i):\n",
    "        file=re.findall(r\"^(.*?)\\.json\", i)[0]\n",
    "        uniquefiles.append(file)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def timestamps(df):\n",
    "    timepress_start=[df.loc[0, \"timepress\"]]\n",
    "    timepress_end=[]\n",
    "    chunkid=0\n",
    "    current_timepress=df.loc[0, \"timepress\"]\n",
    "    current_session=df.loc[0,\"session\"]\n",
    "\n",
    "    for i in range(df[\"keyname\"].shape[0]):\n",
    "        if (df.loc[i, \"chunkid\"] != chunkid):\n",
    "            if current_session != df.loc[i,\"session\"]:\n",
    "                timepress_end.append(df.loc[i-2,\"timepress\"])\n",
    "                current_session=df.loc[i,\"session\"]\n",
    "            else:\n",
    "                timepress_end.append(current_timepress)\n",
    "            timepress_start.append(df.loc[i, \"timepress\"])\n",
    "            current_timepress=df.loc[i, \"timepress\"]\n",
    "            chunkid +=1\n",
    "        current_timepress=df.loc[i, \"timepress\"]\n",
    "        if i ==(len(df)-1):\n",
    "            timepress_end.append(df.loc[i,\"timepress\"])\n",
    "\n",
    "    return timepress_start, timepress_end\n",
    "\n",
    "def recup(df):\n",
    "    current_chunkid=0\n",
    "    session=[]\n",
    "    userid=[]\n",
    "    quizid=[]\n",
    "    chunk=[]\n",
    "    chunkpos=[]\n",
    "    burst_type=[]\n",
    "    \n",
    "    for i in df.index:\n",
    "        if (i==0):\n",
    "            userid.append(df.loc[i,\"userid\"])\n",
    "            session.append(df.loc[i,\"session\"])\n",
    "            # quizid.append(df.loc[i,\"quizid\"])\n",
    "            chunk.append(df.loc[i,\"chunk\"])\n",
    "            burst_type.append(df.loc[i,\"burst_type\"])\n",
    "\n",
    "        if (df.loc[i, \"chunkid\"] != current_chunkid)and i>0:\n",
    "            userid.append(df.loc[i,\"userid\"])\n",
    "            session.append(df.loc[i,\"session\"])\n",
    "            # quizid.append(df.loc[i,\"quizid\"])\n",
    "            chunk.append(df.loc[i,\"chunk\"])\n",
    "            burst_type.append(df.loc[i,\"burst_type\"])\n",
    "            current_chunkid+=1  \n",
    "        \n",
    "    return session,userid,quizid,chunk,burst_type\n",
    "\n",
    "##############################################\n",
    "\n",
    "for filepath in uniquefiles:\n",
    "    #open key level dataset\n",
    "    \n",
    "    df=pd.read_csv(filepath+\"_keylevel.csv\")\n",
    "\n",
    "\n",
    "    timepress_start, timepress_end = timestamps(df)\n",
    "\n",
    "    #part of speech of the words\n",
    "    pos = []\n",
    "    zero=[\"/\"]\n",
    "    for doc in nlp.pipe(df['word'].astype('unicode').values):\n",
    "        if doc.is_parsed:\n",
    "            pos.append([n.pos_ for n in doc])\n",
    "        else:\n",
    "            pos.append(None)\n",
    "        if (pos[-1] == []):\n",
    "            pos[-1] = zero\n",
    "        \n",
    "    pos_new = [i[0] for i in pos]\n",
    "    df['word_pos'] = pos_new\n",
    "\n",
    "    session,userid,quizid,chunk,burst_type=recup(df)\n",
    "    #concatenate the lists into a column\n",
    "\n",
    "    dfw = pd.DataFrame(list(zip(userid, session, chunk, timepress_start, timepress_end,burst_type)),\n",
    "                columns =['userid','session','chunk', 'timepress_start', 'timepress_end','burst_type'])\n",
    "\n",
    "\n",
    "    # todrop=[\"\"]\n",
    "    # dfw = dfw[~dfw['word'].isin(todrop)]\n",
    "    dfw =dfw.reset_index()\n",
    "    dfw\n",
    "    df1=df\n",
    "    df=dfw\n",
    "\n",
    "    def pause_1(df):\n",
    "        df[\"pause_1_after\"]=None\n",
    "        count_row = (df.shape[0]) -2 \n",
    "        current_session=df.loc[0,\"session\"]\n",
    "        for i in range(df[\"chunk\"].shape[0]):\n",
    "            if i <=count_row:\n",
    "                x=df.loc[i,\"timepress_end\"]\n",
    "                y=df.loc[i+1,\"timepress_start\"]\n",
    "                z=y-x\n",
    "                if df.loc[i+1,\"session\"] != current_session:\n",
    "                    z=None\n",
    "                    current_session=df.loc[i+1,\"session\"]\n",
    "                df.loc[i,\"pause_1_after\"]=z\n",
    "        df[\"pause_1_before\"] = None\n",
    "        df.iloc[1:, df.columns.get_loc(\"pause_1_before\")] = df.iloc[:-1, df.columns.get_loc(\"pause_1_after\")].values\n",
    "            \n",
    "    pause_1(df)\n",
    "\n",
    "    def duration(df):\n",
    "        df[\"duration\"]=0\n",
    "        count_row = (df.shape[0]) -1 \n",
    "        for i in range(df[\"chunk\"].shape[0]):\n",
    "            if i <=count_row:\n",
    "                x=df.loc[i,\"timepress_end\"]\n",
    "                y=df.loc[i,\"timepress_start\"]\n",
    "                z=x-y\n",
    "                if z<0:\n",
    "                    z= None\n",
    "                    \n",
    "                df.loc[i,\"duration\"]=z\n",
    "    duration(df)\n",
    "\n",
    "    def nchar(df):\n",
    "        df[\"nchar_chunk\"]=0\n",
    "    #     df[\"nchar_sen\"]=0\n",
    "    #     df[\"nchar_session\"]=0\n",
    "        for i in range(df[\"chunk\"].shape[0]):\n",
    "            count=0\n",
    "            for y in str(df.loc[i,\"chunk\"]):\n",
    "                if y in [ \"[\",\"]\"]:\n",
    "                    count+=1\n",
    "            df.loc[i,\"nchar_chunk\"]=len(str(df.loc[i,\"chunk\"]))-count\n",
    "\n",
    "                    \n",
    "            if df.loc[i,\"burst_type\"]==\"r-burst\":\n",
    "                df.loc[i,\"nchar_chunk\"]=df.loc[i,\"nchar_chunk\"]-2\n",
    "    #         df.loc[i,\"nchar_sentence\"]=len(df.loc[i,\"sentence\"])\n",
    "    #         df.loc[i,\"nchar_session\"]=len(df.loc[i,\"session_text\"])\n",
    "\n",
    "    nchar(df)\n",
    "\n",
    "\n",
    "    def char_speed_chunk(df):\n",
    "        df[\"char_speed_chunk\"]=0\n",
    "        for i in range(df[\"chunk\"].shape[0]):\n",
    "            if df.loc[i,\"duration\"] != 0:\n",
    "                df.loc[i,\"char_speed_chunk\"]=((df.loc[i,\"nchar_chunk\"])/(df.loc[i,\"duration\"]))*1000\n",
    "    char_speed_chunk(df)\n",
    "\n",
    "    from nltk.corpus import words\n",
    "\n",
    "    pos = []\n",
    "    zero=[\"/\"]\n",
    "\n",
    "    df[\"left_boundary_pos\"]=0\n",
    "    df[\"right_boundary_pos\"]=0\n",
    "\n",
    "    for doc in nlp.pipe(df['chunk'].astype('unicode').values):\n",
    "        if doc.is_parsed:\n",
    "            pos.append([n.pos_ for n in doc])\n",
    "        else:\n",
    "            pos.append(None)\n",
    "        if (pos[-1] == []):\n",
    "            pos[-1] = zero\n",
    "            \n",
    "        \n",
    "        \n",
    "    for i in range(len(pos)):\n",
    "        df.loc[i,\"left_boundary_pos\"]=pos[i][0]\n",
    "        df.loc[i,\"right_boundary_pos\"]=pos[i][-1]\n",
    "\n",
    "\n",
    "    def chunk_boundary(df):\n",
    "        df[\"left_boundary_chunk\"]=0\n",
    "        df['left_boundary_chunk'] = df['chunk'].str.split().str[0].str.strip()\n",
    "        df[\"right_boundary_chunk\"]=0\n",
    "        df['right_boundary_chunk'] = df['chunk'].str.split().str[-1].str.strip()\n",
    "    #     df[\"left_boundary_pos\"]=0\n",
    "    #     df['left_boundary_pos'] = df['chunkpos'].str.split().str[0].str.strip()\n",
    "    #     df[\"right_boundary_pos\"]=0\n",
    "    #     df['right_boundary_pos'] = df['chunkpos'].str.split().str[-1].str.strip()\n",
    "\n",
    "    chunk_boundary(df)\n",
    "\n",
    "\n",
    "    suffix=[\"eer\",\"er\",\"ion\",\"ity\",\"ment\",\"ness\",\"or\",\"sion\",\"ship\",\"th\",\"able\",\"ible\",\"al\",\"ant\",\"ary\",\"ful\",\"ic\",\"ious\", \"ous\", \"ive\", \"less\",\"y\",\"ed\",\"en\",\"er\",\"ing\",\"ize\",\"ise\",\"ly\",\"ward\",\"wise\",\"s\"]\n",
    "    df[\"left_suffix\"]=0\n",
    "    for i in df.index:\n",
    "        if df.loc[i,\"left_boundary_chunk\"] in suffix:\n",
    "            df.loc[i,\"left_suffix\"]=1\n",
    "\n",
    "\n",
    "    from nltk.corpus import words\n",
    "    import re\n",
    "\n",
    "    df[\"incomplete_right\"]=0\n",
    "    df[\"incomplete_left\"]=0\n",
    "    for i in df.index:\n",
    "        if i < len(df)-1:\n",
    "            if df.loc[i,\"burst_type\"]==\"p-burst\":\n",
    "                x=str(df.loc[i,\"right_boundary_chunk\"])+str(df.loc[i+1,\"left_boundary_chunk\"])\n",
    "                if x.lower() in words.words():\n",
    "                    df.loc[i,\"incomplete_right\"]=1\n",
    "                    df.loc[i+1,\"incomplete_left\"]=1\n",
    "                    x=\"\"\n",
    "            if i<len(df)-2 and df.loc[i,\"burst_type\"]==\"r-burst\":\n",
    "                y=str(df.loc[i,\"right_boundary_chunk\"])\n",
    "                y=re.sub(r'[^\\w]', ' ', y)\n",
    "                x=y+str(df.loc[i+2,\"left_boundary_chunk\"])\n",
    "                if x.lower() in words.words():\n",
    "                    df.loc[i,\"incomplete_right\"]=1\n",
    "                    df.loc[i+2,\"incomplete_left\"]=1\n",
    "                    x=\"\"\n",
    "    punct=['\"\"',\"''\",'.',',','?','!',':',';',\"-\"] #à completer\n",
    "    df[\"punct_right\"]=0\n",
    "    df[\"punct_left\"]=0\n",
    "    for i in df.index:\n",
    "        if df.loc[i,\"right_boundary_chunk\"] in punct:\n",
    "            df.loc[i,\"punct_right\"]=1\n",
    "        if df.loc[i,\"left_boundary_chunk\"] in punct:\n",
    "            df.loc[i,\"punct_left\"]=1\n",
    "\n",
    "\n",
    "    df[\"burst_type_before\"]=0\n",
    "    df.iloc[1:, df.columns.get_loc(\"burst_type_before\")] = df.iloc[:-1, df.columns.get_loc(\"burst_type\")].values\n",
    "\n",
    "\n",
    "\n",
    "    df[\"right_boundary_summary\"]=0\n",
    "    df[\"left_boundary_summary\"]=0\n",
    "    for i in df.index:\n",
    "        if df.loc[i,\"right_boundary_pos\"]==\"SPACE\" :\n",
    "            df.loc[i,\"right_boundary_summary\"]=df.loc[i,\"right_boundary_pos\"]\n",
    "        elif df.loc[i,\"incomplete_right\"]==1:\n",
    "            df.loc[i,\"right_boundary_summary\"]=\"INCOMPLETE\"\n",
    "        elif df.loc[i,\"punct_right\"]==1:\n",
    "            df.loc[i,\"right_boundary_summary\"]=\"PUNCT\"    \n",
    "        else:\n",
    "            df.loc[i,\"right_boundary_summary\"]=\"WORD\"\n",
    "        if df.loc[i,\"left_boundary_pos\"]==\"SPACE\": \n",
    "            df.loc[i,\"left_boundary_summary\"]=df.loc[i,\"left_boundary_pos\"]\n",
    "        elif df.loc[i,\"incomplete_left\"]==1:\n",
    "            df.loc[i,\"left_boundary_summary\"]=\"INCOMPLETE\"\n",
    "        elif df.loc[i,\"punct_left\"]==1:\n",
    "            df.loc[i,\"left_boundary_summary\"]=\"PUNCT\"    \n",
    "        else:\n",
    "            df.loc[i,\"left_boundary_summary\"]=\"WORD\"\n",
    "\n",
    "    df[\"chunk_before\"]=0\n",
    "    df.iloc[1:, df.columns.get_loc(\"chunk_before\")] = df.iloc[:-1, df.columns.get_loc(\"chunk\")].values\n",
    "    df[\"chunk_after\"]=0\n",
    "    df.iloc[:-1, df.columns.get_loc(\"chunk_after\")] = df.iloc[1:, df.columns.get_loc(\"chunk\")].values\n",
    "                \n",
    "    # ## add the sentence pos\n",
    "    pos = []\n",
    "    for doc in nlp.pipe(df['chunk'].astype('unicode').values):\n",
    "        if doc.is_parsed:\n",
    "            pos.append([n.pos_ for n in doc])\n",
    "                    \n",
    "            for i in range(len(pos)):\n",
    "                if df.loc[i,\"incomplete_right\"]==1:\n",
    "                    pos[i][-1]=\"INCOMPLETE\"\n",
    "                if df.loc[i,\"incomplete_left\"]==1  :\n",
    "                    pos[i][0]= \"INCOMPLETE\"\n",
    "        else:\n",
    "            pos.append(None)\n",
    "        if (pos[-1] == []):\n",
    "            pos[-1] = zero\n",
    "    df['chunkpos'] = pos\n",
    "\n",
    "    def split(word): \n",
    "        return [char for char in word] \n",
    "\n",
    "    def flag_pos_chunk(df):\n",
    "        df[\"VERB_in_chunk\"]=0\n",
    "        df[\"NOUN_in_chunk\"]=0\n",
    "        df[\"ADJ_in_chunk\"]=0\n",
    "        df[\"ADV_in_chunk\"]=0\n",
    "        df[\"PRON_in_chunk\"]=0\n",
    "        for i in df.index:\n",
    "            if any(ext in  str(df.loc[i,'chunkpos']) for ext in ['VERB']):\n",
    "                df.loc[i,\"VERB_in_chunk\"]=1\n",
    "            if any(ext in  str(df.loc[i,'chunkpos']) for ext in ['NOUN']):\n",
    "                df.loc[i,\"NOUN_in_chunk\"]=1\n",
    "            if any(ext in  str(df.loc[i,'chunkpos']) for ext in ['ADJ']):\n",
    "                df.loc[i,\"ADJ_in_chunk\"]=1\n",
    "            if any(ext in  str(df.loc[i,'chunkpos']) for ext in ['ADV']):\n",
    "                df.loc[i,\"ADV_in_chunk\"]=1            \n",
    "            if any(ext in  str(df.loc[i,'chunkpos']) for ext in ['PRON']):\n",
    "                df.loc[i,\"PRON_in_chunk\"]=1              \n",
    "\n",
    "\n",
    "\n",
    "    flag_pos_chunk(df)       \n",
    "\n",
    "    def cleanup(df):\n",
    "        for i in df.index:\n",
    "            df.loc[i,\"right_boundary_chunk\"]=str(df.loc[i,\"right_boundary_chunk\"]).replace(\"[\",\"\")\n",
    "            df.loc[i,\"right_boundary_chunk\"]=str(df.loc[i,\"right_boundary_chunk\"]).replace(\"]\",\"\")\n",
    "            df.loc[i,\"left_boundary_chunk\"]=str(df.loc[i,\"left_boundary_chunk\"]).replace(\"[\",\"\")\n",
    "            df.loc[i,\"left_boundary_chunk\"]=str(df.loc[i,\"left_boundary_chunk\"]).replace(\"]\",\"\")\n",
    "            df.loc[i,\"left_boundary_chunk\"]=str(df.loc[i,\"left_boundary_chunk\"]).replace(\"nan\",\"\")\n",
    "            df.loc[i,\"right_boundary_chunk\"]=str(df.loc[i,\"right_boundary_chunk\"]).replace(\"nan\",\"\")\n",
    "\n",
    "\n",
    "    cleanup(df)\n",
    "\n",
    "\n",
    "    punct=['\"\"',\"''\",'.',',','?','!',':',';',\"-\"] #à completer\n",
    "\n",
    "    def punctuation_side(df):\n",
    "        df[\"punct_right\"]=0\n",
    "        df[\"punct_left\"]=0\n",
    "        df[\"first_char_punct\"]=0\n",
    "        df[\"last_char_punct\"]=0\n",
    "        for i in df.index:\n",
    "    # \n",
    "\n",
    "            if df.loc[i,\"right_boundary_chunk\"] in punct:\n",
    "                df.loc[i,\"punct_right\"]=1\n",
    "                    \n",
    "            if df.loc[i,\"left_boundary_chunk\"] in punct:\n",
    "                df.loc[i,\"punct_left\"]=1\n",
    "                    \n",
    "            if len(str(df.loc[i,\"right_boundary_chunk\"])) > 0 :\n",
    "                if str(df.loc[i,\"right_boundary_chunk\"])[-1] in punct:\n",
    "                    df.loc[i,\"last_char_punct\"]=str(df.loc[i,\"right_boundary_chunk\"])[-1]\n",
    "                    df.loc[i,\"punct_right\"]=1\n",
    "                    \n",
    "                    \n",
    "            if len(str(df.loc[i,\"left_boundary_chunk\"])) > 0:\n",
    "                if str(df.loc[i,\"left_boundary_chunk\"])[0] in punct:\n",
    "                    df.loc[i,\"first_char_punct\"]=str(df.loc[i,\"left_boundary_chunk\"])[0]\n",
    "                    df.loc[i,\"punct_left\"]=1\n",
    "\n",
    "    punctuation_side(df)\n",
    "\n",
    "    def summary_chunk_boundary(df):\n",
    "        df[\"right_boundary_summary\"]=0\n",
    "        df[\"left_boundary_summary\"]=0\n",
    "        for i in df.index:\n",
    "            if df.loc[i,\"right_boundary_pos\"]==\"SPACE\" :\n",
    "                df.loc[i,\"right_boundary_summary\"]=df.loc[i,\"right_boundary_pos\"]\n",
    "            elif df.loc[i,\"incomplete_right\"]==1:\n",
    "                df.loc[i,\"right_boundary_summary\"]=\"INCOMPLETE\"\n",
    "            elif df.loc[i,\"punct_right\"]==1:\n",
    "                df.loc[i,\"right_boundary_summary\"]=\"PUNCT\"    \n",
    "            else:\n",
    "                df.loc[i,\"right_boundary_summary\"]=\"WORD\"\n",
    "            if df.loc[i,\"left_boundary_pos\"]==\"SPACE\": \n",
    "                df.loc[i,\"left_boundary_summary\"]=df.loc[i,\"left_boundary_pos\"]\n",
    "            elif df.loc[i,\"incomplete_left\"]==1:\n",
    "                df.loc[i,\"left_boundary_summary\"]=\"INCOMPLETE\"\n",
    "            elif df.loc[i,\"punct_left\"]==1:\n",
    "                df.loc[i,\"left_boundary_summary\"]=\"PUNCT\"    \n",
    "            else:\n",
    "                df.loc[i,\"left_boundary_summary\"]=\"WORD\"\n",
    "\n",
    "    # summary_chunk_boundary(dfC)\n",
    "    summary_chunk_boundary(df)\n",
    "\n",
    "\n",
    "    def split(word): \n",
    "        return [char for char in word]  \n",
    "    ##First step : subset by user\n",
    "\n",
    "    #make a list of punctuation symbols :\n",
    "    punct=['\"\"',\"''\",'.',',','?','!',':',';'] \n",
    "\n",
    "    def punct_middle(dfC):\n",
    "        dfC[\"punct_middle\"]=0\n",
    "        dfC[\"punct_amount\"]=0\n",
    "\n",
    "        for i in dfC.index:\n",
    "            counter=0\n",
    "            real_count=0\n",
    "            if any(ext in  split(str(dfC.loc[i,'chunk'])) for ext in punct):\n",
    "                for j in split(str(dfC.loc[i,'chunk'])):\n",
    "                    if j in punct:\n",
    "                        real_count+=1\n",
    "\n",
    "                    #update real count\n",
    "\n",
    "                #Check if the punctuation mark is at the boundary of the chunk\n",
    "                    if dfC.loc[i,\"punct_right\"]==1:\n",
    "                        counter+=1\n",
    "                    if dfC.loc[i,\"punct_left\"]==1:\n",
    "                        counter+=1\n",
    "                    if real_count > counter:\n",
    "                        dfC.loc[i,\"punct_middle\"]=1\n",
    "                    \n",
    "                    dfC.loc[i,\"punct_amount\"]= real_count\n",
    "                    \n",
    "    punct_middle(df)\n",
    "\n",
    "    def punct_position(df):\n",
    "        df[\"punct_position\"]=\"\"\n",
    "        for i in df.index :\n",
    "            if df.loc[i,\"punct_left\"]==1:\n",
    "                df.loc[i,\"punct_position\"]=\"left\"\n",
    "            if df.loc[i,\"punct_middle\"]==1:\n",
    "                df.loc[i,\"punct_position\"]=\"middle\"\n",
    "            if df.loc[i,\"punct_right\"]==1:\n",
    "                df.loc[i,\"punct_position\"]=\"right\"\n",
    "                \n",
    "    punct_position(df)\n",
    "\n",
    "    def chunk_word_count(df):\n",
    "        df[\"word_count\"]=\"\"\n",
    "        for i in df.index:\n",
    "            df.loc[i,\"word_count\"] = len(str(df.loc[i,\"chunk\"]).split()) \n",
    "            \n",
    "\n",
    "    chunk_word_count(df)\n",
    "    def burst_type_count(df):\n",
    "        df[\"p_burst_index\"]=0\n",
    "        df[\"r_burst_index\"]=0\n",
    "        for i in df.index:\n",
    "            if df.loc[i,\"burst_type\"]==\"p-burst\":\n",
    "                    df.loc[i,\"p_burst_index\"]=1\n",
    "            if df.loc[i,\"burst_type\"]==\"r-burst\":\n",
    "                    df.loc[i,\"r_burst_index\"]=1\n",
    "                \n",
    "    burst_type_count(df)\n",
    "\n",
    "    df.to_csv(str(filepath)+'_chunklevel.csv',index=False,encoding=\"utf-8\")\n",
    "\n",
    "    df1=df\n",
    "    text=\"\"\n",
    "    chunks=[]\n",
    "    pause=[]\n",
    "    current_chunk=\"\"\n",
    "    chunks.append(df1.loc[0,\"chunk\"])\n",
    "    chunkid=df1.loc[0,\"index\"]\n",
    "    for i in df1.index:\n",
    "        if df1.loc[i,\"index\"] != chunkid:\n",
    "            if str(df1.loc[i,\"chunk\"])==\"nan\":\n",
    "                chunks.append(\"[backspace]\")\n",
    "            else:\n",
    "                chunks.append(df1.loc[i,\"chunk\"])\n",
    "            pause.append(df1.loc[i,\"pause_1_before\"])\n",
    "            chunkid +=1\n",
    "    counter=0\n",
    "    for i in range(len(chunks)-1):\n",
    "        a=str(chunks[i])\n",
    "        b=str(pause[i])\n",
    "        #with numbers\n",
    "        # c= str(counter)+\" - \"+a + \" \"+(\"[\"+ b +\"]\").rjust(10)+\"\\n\"\n",
    "        #without numbers\n",
    "        c= a + \" \"+(\"[\"+ b +\"]\").rjust(10)+\"\\n\"\n",
    "\n",
    "        # print(c, sep=' ', end='', flush=True)\n",
    "        text=text+c\n",
    "        counter+=1\n",
    "    # print(str(counter)+\" - \"+str(chunks[-1]))\n",
    "    # text=text + c\n",
    "    # print(text)\n",
    "\n",
    "    textfile = open(str(filepath)+\"-copie_chunk\"+'.txt', 'w')\n",
    "    textfile.write(text)\n",
    "    textfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "94ecff09fac4bbb4628c9d5f61ebbc3a373b057e868617667b1031cb407844b9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
